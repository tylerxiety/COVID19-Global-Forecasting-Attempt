{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Id    Population         Weight    TargetValue\n",
      "count  844972.000000  8.449720e+05  844972.000000  844972.000000\n",
      "mean   484802.500000  2.720127e+06       0.530870      10.959682\n",
      "std    279911.120335  3.477771e+07       0.451909     275.699242\n",
      "min         1.000000  8.600000e+01       0.047491  -10034.000000\n",
      "25%    242383.750000  1.213300e+04       0.096838       0.000000\n",
      "50%    484802.500000  3.053100e+04       0.349413       0.000000\n",
      "75%    727221.250000  1.056120e+05       0.968379       0.000000\n",
      "max    969604.000000  1.395773e+09       2.239186   36163.000000\n",
      "-----------------------------------------------------------------\n",
      "            County Province_State Country_Region        Date          Target\n",
      "count       766892         799344         844972      844972          844972\n",
      "unique        1840            133            187         122               2\n",
      "top     Washington          Texas             US  2020-05-16  ConfirmedCases\n",
      "freq          7564          62220         780312        6926          422486\n",
      "-----------------------------------------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 844972 entries, 0 to 844971\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count   Dtype  \n",
      "---  ------          --------------   -----  \n",
      " 0   Id              844972 non-null  int64  \n",
      " 1   County          766892 non-null  object \n",
      " 2   Province_State  799344 non-null  object \n",
      " 3   Country_Region  844972 non-null  object \n",
      " 4   Population      844972 non-null  int64  \n",
      " 5   Weight          844972 non-null  float64\n",
      " 6   Date            844972 non-null  object \n",
      " 7   Target          844972 non-null  object \n",
      " 8   TargetValue     844972 non-null  float64\n",
      "dtypes: float64(2), int64(2), object(5)\n",
      "memory usage: 58.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# greet the data\n",
    "print(df.describe())\n",
    "print(\"-\"*65)\n",
    "print(df.describe(include=['O']))\n",
    "print(\"-\"*65)\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and testing datasets and actual labels\n",
    "train = df.copy()\n",
    "train = train[train.Date <= '2020-05-10']\n",
    "\n",
    "test = df.copy()\n",
    "test = test[(test['Date']>='2020-05-11')&(test['Date']<='2020-05-17')]\n",
    "\n",
    "actuals = test.TargetValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# seems like the data is cleaned\\ntrain.Country_Region.unique().tolist()\\ntrain.County.unique().tolist()\\ntrain.Province_State.unique().tolist()'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# seems like the data is cleaned\n",
    "train.Country_Region.unique().tolist()\n",
    "train.County.unique().tolist()\n",
    "train.Province_State.unique().tolist()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# See the training data\\nprint(train.describe())\\nprint(\"-\"*65)\\nprint(train.describe(include=[\\'O\\']))\\nprint(\"-\"*65)\\nprint(train.info())'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# See the training data\n",
    "print(train.describe())\n",
    "print(\"-\"*65)\n",
    "print(train.describe(include=['O']))\n",
    "print(\"-\"*65)\n",
    "print(train.info())'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of ideas for models:\n",
    "- quntile linear regression from mlinsights\n",
    "- quantile gradient boosting regressor from sklearn\n",
    "- quantile regression forests from skargen\n",
    "- quantile deep net using Keras or Tensorflow\n",
    "\n",
    "A couple of ideas for the missing values:\n",
    "- drop \n",
    "- impute\n",
    "- impute with mark on which ones are missing\n",
    "\n",
    "A couple of ideas for the columns:\n",
    "\n",
    "- County: drop or keep\n",
    "- Province_State: drop or keep\n",
    "- Population: bins or as is\n",
    "- Date: day of week, day of month, month, day of year\n",
    "\n",
    "- relationship b/w confirmed and death?\n",
    "\n",
    "- New columns:Continuent ?\n",
    "\n",
    "Ways of encoding\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the Weighted Pinball Loss function\n",
    "\n",
    "def PLoss(preds, actuals, qt):\n",
    "    loss = np.maximum((actuals-preds)*qt, (preds-actuals)*(1-qt))\n",
    "    return loss\n",
    "\n",
    "def WPLoss(preds05, preds50, preds95, actuals, weights):\n",
    "    l05 = PLoss(preds05, actuals,0.05)\n",
    "    l50 = PLoss(preds50, actuals,0.5)\n",
    "    l95 = PLoss(preds95, actuals,0.95)\n",
    "    score = sum((l05+l50+l95)/3*weights)/actuals.count()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create label y\n",
    "y = train.TargetValue\n",
    "\n",
    "# Clean train and test data\n",
    "train = train.drop(['Id','TargetValue'],axis=1)\n",
    "testId = test.Id\n",
    "test = test.drop(['Id','TargetValue'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#date\n",
    "\n",
    "train.Date = pd.to_datetime(train.Date)\n",
    "test.Date = pd.to_datetime(test.Date)\n",
    "\n",
    "for ds in [train, test]:\n",
    "  ds['Day'] = ds.Date.dt.day\n",
    "  ds['Month'] = ds.Date.dt.month\n",
    "  ds['Dayofweek'] = ds.Date.dt.dayofweek\n",
    "  ds['Dayofyear'] = ds.Date.dt.dayofyear\n",
    "  ds = ds.drop(['Date'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns with missing values for now\n",
    "train_noMis = train.copy()\n",
    "test_noMis = test.copy()\n",
    "\n",
    "train_noMis = train_noMis.drop(['County','Province_State','Date'],axis=1)\n",
    "test_noMis = test_noMis.drop(['County','Province_State','Date'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train data into validation and training data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_X, val_X, train_y, val_y = train_test_split(train_noMis, y,random_state=1, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the models with droping the columns with missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode Country_Region and Target\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "train_Xen = train_X.copy()\n",
    "val_Xen = val_X.copy()\n",
    "\n",
    "for col in ['Country_Region','Target']:\n",
    "  train_Xen[col] = label_encoder.fit_transform(train_X[col])\n",
    "  val_Xen[col] = label_encoder.transform(val_X[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not sure what the 'quantile' loss function in GBR!\n",
    "\n",
    "#GradientBoostingRegressor\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "def gbrPred(train_X, val_X, train_y,val_y,qt):\n",
    "    model = GradientBoostingRegressor(loss='quantile', alpha=qt)\n",
    "    model.fit(train_X,train_y)\n",
    "    \n",
    "    preds = model.predict(val_X)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gbr05 = gbrPred(train_Xen, val_Xen, train_y,val_y,0.05)\n",
    "gbr50 = gbrPred(train_Xen, val_Xen, train_y,val_y,0.5)\n",
    "gbr95 = gbrPred(train_Xen, val_Xen, train_y,val_y,0.95)\n",
    "\n",
    "gbr_score = WPLoss(gbr05, gbr50, gbr95, val_y, val_X.Weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr05 = gbrPred(train_Xen, val_Xen, train_y,val_y,0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr50 = gbrPred(train_Xen, val_Xen, train_y,val_y,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr95 = gbrPred(train_Xen, val_Xen, train_y,val_y,0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_score = WPLoss(gbr05, gbr50, gbr95, val_y, val_X.Weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbr05:\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "gbr50:\n",
      "[0.   0.   0.   ... 0.   0.   0.19]\n",
      "gbr95:\n",
      "[1.77795138e-01 8.33825906e-01 4.65102475e+00 ... 3.23508112e+02\n",
      " 6.82135758e-01 2.71198918e+00]\n",
      "gbr weighted score:\n",
      "0.2999235618524082\n"
     ]
    }
   ],
   "source": [
    "print('gbr05:')\n",
    "print(gbr05)\n",
    "print('gbr50:')\n",
    "print(gbr50)\n",
    "print('gbr95:')\n",
    "print(gbr95)\n",
    "print('gbr weighted score:')\n",
    "print(gbr_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantile linear regression\n",
    "#from mlinsights.mlmodel import QuantileLinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep nets\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
